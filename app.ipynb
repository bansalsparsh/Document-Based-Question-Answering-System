{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing important libraries"
      ],
      "metadata": {
        "id": "-BebUFcf_Xcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit langchain langchain-google-genai langchain-community pyngrok python-dotenv --quiet faiss-cpu --quiet"
      ],
      "metadata": {
        "id": "Uicz1Hbp_KCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Script being written in a file"
      ],
      "metadata": {
        "id": "Odzvc0M5_eAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# Securing API key as input\n",
        "gemini_api_key = getpass(\"üîê Enter your Gemini API key: \")\n",
        "\n",
        "# Writing Streamlit code to a file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(f'''\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain.schema import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Set the Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"{gemini_api_key}\"\n",
        "\n",
        "# Initialize LLM and embedding\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
        "\n",
        "# Function to load and prepare document data\n",
        "def load_and_prepare_data(document_text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    doc = Document(page_content=document_text)\n",
        "    document_chunks = text_splitter.split_documents([doc])\n",
        "    vector_store_faiss = FAISS.from_documents(document_chunks, embedding)\n",
        "    return vector_store_faiss\n",
        "\n",
        "# Streamlit UI\n",
        "st.title('üìÑ Information Query System (Gemini-Powered)')\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a .txt file\", type=\"txt\")\n",
        "if uploaded_file is not None:\n",
        "    document_text = uploaded_file.read().decode(\"utf-8\")\n",
        "    vector_store_faiss = load_and_prepare_data(document_text)\n",
        "\n",
        "    query = st.text_input(\"Enter your question:\")\n",
        "\n",
        "    if st.button(\"üîç Get Answer\") and query:\n",
        "        result = vector_store_faiss.similarity_search(query, k=3)\n",
        "        context = \" \".join([doc.page_content for doc in result])\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(\\\"\\\"\\\"\n",
        "        Answer the following question using only the context provided.\n",
        "        <context>\n",
        "        {{context}}\n",
        "        </context>\n",
        "        \\\"\\\"\\\")\n",
        "\n",
        "        document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "        retriever = vector_store_faiss.as_retriever()\n",
        "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "        response = retrieval_chain.invoke({{\"input\": query, \"context\": context}})\n",
        "        answer = response.get('answer', '‚ùå No answer found.')\n",
        "\n",
        "        st.markdown(\"### üß† Answer:\")\n",
        "        st.write(answer)\n",
        "''')"
      ],
      "metadata": {
        "id": "pyjmnGZ__RXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using ngrok for creating public URLs for local apps"
      ],
      "metadata": {
        "id": "6-3nz16dAQ7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "authtoken = getpass(\"üîê Enter your ngrok authtoken: \")\n",
        "# Adding authtoken for using pyngrok\n",
        "!ngrok config add-authtoken {authtoken}\n",
        "\n",
        "# Kill existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Create public tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"üöÄ Streamlit app is live at: {public_url}\")\n",
        "\n",
        "# Run Streamlit in background\n",
        "!streamlit run app.py &>/dev/null &\n",
        "time.sleep(2)  # Wait a bit to make sure the app launches"
      ],
      "metadata": {
        "id": "Vz_KDb2i_SMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}